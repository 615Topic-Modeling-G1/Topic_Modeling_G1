---
title: "Topic Modeling Runci"
author: "Runci Hu"
date: "2022-11-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
library(dplyr)
library(stringr)
library(tidyr)
library(tidytext)
library(tidyverse)
library(topicmodels)
library(stopwords)
library(igraph)
library(ggraph)
library(ggplot2)
library(DescTools)
library(widyr)
```

```{r}
imdb <- read.csv("~/Desktop/IMDB Dataset.csv")
```

```{r}
imdb <- imdb %>% select(-sentiment)
imdb_df <- tibble(review = 1:50000, sentence = imdb[,1])
imdb_df <- imdb_df[1:1000,]
text_df <- imdb_df %>% slice_sample(n = 100, replace = FALSE)
```

# Bigram
```{r}
token_bigram <- text_df %>%
  unnest_tokens(bigram,
                sentence, 
                token = "ngrams",
                n = 2,
                to_lower=TRUE) %>%
  count(review,bigram,sort = TRUE)%>%
  filter(!is.na(bigram))

## create a stop word vector, drop the attribute, drop the attribute
stop <-  unlist(stop_words[,1])
stop <- StripAttr(stop)
stop <- c(stop, "br")

## split the bigram list into two columns
check <-  token_bigram %>% separate(bigram, 
                                    sep= " ", 
                                    c("w1", "w2"))

## check both words individually agains stop word lists
a <- check$w1 %in% stop
b <- check$w2 %in% stop
## the bigram is included only if neither of the single words is a stop word
remove <- (a|b)
## to make it easier to see create a data frame
d <- cbind(token_bigram, a, b, remove)

d <- d %>% filter(d$a !="br" && d$b != "br")

## create an index of bigram
f <- which(d$remove == FALSE)
## use the index to make a list of bigrams
g <- d$bigram[f]

(review_separated <- token_bigram %>%  
  separate(bigram, into = c("word1", "word2"), sep = " ")
)

review_united <- review_separated %>%
  filter(!word1 %in% c('br'),
         !word2 %in% c('br')) %>%
  unite(bigram, c(word1, word2), sep = " ")

total_bigram <- review_united %>%
  group_by(review) %>%
  summarize(total = sum(n))

review_bigram <- left_join(review_united, total_bigram)
rm(token_bigram, review_separated, review_united, total_bigram)
```

# frequency 
```{r}
freq_by_rank_bi <- review_bigram %>% 
  group_by(review) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup()

freq_by_rank_bi %>% 
  ggplot(aes(rank, `term frequency`, color = review)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```
From this graph, we can see that the word frequency has a decrease tendency. 
By this term frequency graph, we can choose words with the highest frequency 
and consider them as stop words. The following tf-idf is the method we test
bigrams and find stop words.
# tf-idf
```{r}
review_tf_idf_bi <- review_bigram %>%
  bind_tf_idf(bigram, review, review)
#look at terms with high tf-idf in reviews.
review_tf_idf_bi <- review_tf_idf_bi %>%
  select(-total) %>%
  arrange(desc(tf-idf))

head(review_tf_idf_bi)
```

# select bigram stop words 
```{r}
stopwords <- as.vector(review_tf_idf_bi$bigram)
u1 <- unique(stopwords)
stopwords <- data.frame(u1)
sw <- as.character(stopwords$u1[1:10000])
sw <- tibble(sw)
head(sw)
```
# single-word stop words
```{r}
imdb <- imdb  %>%  mutate(docs = c(1:length(imdb$review)))
data(stop_words)
stop_words <- data.frame(stop_words$word)
stop_words <- rbind(stop_words, "br", "movie", "film", "movies", "films", "scenes", "scene", "character", "characters", "watch", "watching")
colnames(stop_words) <- c("word")
```
For the LDA, we choose to use single-word stop words, because they learn beta, 
the per-topic-per-word probabilities, from the text book.

# LDA
```{r}
imdb_dtm <- imdb %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words)%>%
  count(docs, word) %>%
  cast_dtm(docs, word, n)

imdb_lda <- LDA(imdb_dtm, k = 10, control = list(seed = 2022))
imdb_topics <- tidy(imdb_lda, matrix = "beta")
imdb_topics

imdb_top_terms <- imdb_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

imdb_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

beta_wide <- imdb_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))
```

# Document Classification
```{r}
imdb_documents <- tidy(imdb_lda, matrix = "gamma")
# check the per-document-per-topic probabilities using gamma
imdb_documents <- imdb_documents %>%
  separate(document, c("title"),sep = "_", convert = TRUE)
head(imdb_documents)
```

```{r, warning=FALSE, message=FALSE}
ggplot(imdb_documents, aes(x = gamma , fill = as.factor(topic))) + 
  geom_histogram()+
  facet_wrap(~topic, ncol = 3) + 
  scale_y_log10() +
  labs(title = "per-document-per-topic probabilities",
       y = "documents number", x= "gamma")
```


```{r}
ggplot(imdb_documents, aes(factor(topic),gamma )) + 
  geom_boxplot() +
  labs(title = "per-document-per-topic probabilities",
       y = "gamma", x= "topic")
```


```{r, warning=FALSE, message=FALSE}
imdb_title <- imdb %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words) %>%
  count(docs, word, sort = TRUE)

imdb_total <-imdb_title %>% 
  group_by(docs) %>% 
  summarize(total = sum(n))

imdb_title <- left_join(imdb_title,imdb_total)

imdb_title_pair <- imdb_title %>% 
  pairwise_count(word, docs, sort = TRUE, upper = FALSE)

imdb_title_pair %>%
  filter(n >= 2000) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "royalblue") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```


```{r, warning=FALSE, message=FALSE}
imdb_title_cor <- imdb_title %>% 
  group_by(docs) %>%
  filter(n() >= 500) %>%
  pairwise_cor(word, docs, sort = TRUE, upper = FALSE)

imdb_cor <- imdb_title_cor[1:1000,]

set.seed(2022)
imdb_cor %>%
  filter(correlation > .9) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "royalblue") +
  geom_node_point(size = 3) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```















